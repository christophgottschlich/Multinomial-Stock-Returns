{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080b3b8c",
   "metadata": {},
   "source": [
    "In the following section, we are searching for the optimal hyperparameters needed in each model. Therefore, we are using the Grid-Search-Method with cross-validation which we get from the package sklearn. We reduce the number of Parameters and use default Parameters for some models in order to obtain better executability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53493907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from keras.models import Sequential\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras import regularizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b7d8a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>USTB3Md_t</th>\n",
       "      <th>Zt</th>\n",
       "      <th>Class_p</th>\n",
       "      <th>WILL_t</th>\n",
       "      <th>RABS_WIL_t</th>\n",
       "      <th>R_WIL_t</th>\n",
       "      <th>SQR_WIL_t</th>\n",
       "      <th>DAX_t</th>\n",
       "      <th>R_DAX_t</th>\n",
       "      <th>...</th>\n",
       "      <th>UR_t</th>\n",
       "      <th>R_UR_t</th>\n",
       "      <th>USGDP_t</th>\n",
       "      <th>R_USGDP_t</th>\n",
       "      <th>VIX_t</th>\n",
       "      <th>Unnamed: 41</th>\n",
       "      <th>Unnamed: 42</th>\n",
       "      <th>Unnamed: 43</th>\n",
       "      <th>Unnamed: 44</th>\n",
       "      <th>Unnamed: 45</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991-01-01 00:00:00</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>-0.017722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3101.36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1398.23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9275.276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991-01-02 00:00:00</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>-0.022195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3069.58</td>\n",
       "      <td>-0.010247</td>\n",
       "      <td>-0.004473</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>1366.10</td>\n",
       "      <td>-0.010096</td>\n",
       "      <td>...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9275.276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1991-01-03 00:00:00</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>-0.022967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3032.73</td>\n",
       "      <td>-0.012005</td>\n",
       "      <td>-0.005245</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>1366.65</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9275.276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1991-01-04 00:00:00</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>-0.018856</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3024.82</td>\n",
       "      <td>-0.002608</td>\n",
       "      <td>-0.001134</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1396.07</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9275.276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991-01-07 00:00:00</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>-0.025084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2973.98</td>\n",
       "      <td>-0.016808</td>\n",
       "      <td>-0.007361</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>1358.16</td>\n",
       "      <td>-0.011956</td>\n",
       "      <td>...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9275.276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DATE  USTB3Md_t        Zt  Class_p   WILL_t  RABS_WIL_t  \\\n",
       "0  1991-01-01 00:00:00   0.017722 -0.017722      1.0  3101.36    0.000000   \n",
       "1  1991-01-02 00:00:00   0.017722 -0.022195      1.0  3069.58   -0.010247   \n",
       "2  1991-01-03 00:00:00   0.017722 -0.022967      1.0  3032.73   -0.012005   \n",
       "3  1991-01-04 00:00:00   0.017722 -0.018856      1.0  3024.82   -0.002608   \n",
       "4  1991-01-07 00:00:00   0.017722 -0.025084      1.0  2973.98   -0.016808   \n",
       "\n",
       "    R_WIL_t  SQR_WIL_t    DAX_t   R_DAX_t  ...  UR_t  R_UR_t   USGDP_t  \\\n",
       "0  0.000000   0.000000  1398.23  0.000000  ...   6.4     0.0  9275.276   \n",
       "1 -0.004473   0.000020  1366.10 -0.010096  ...   6.4     0.0  9275.276   \n",
       "2 -0.005245   0.000028  1366.65  0.000175  ...   6.4     0.0  9275.276   \n",
       "3 -0.001134   0.000001  1396.07  0.009250  ...   6.4     0.0  9275.276   \n",
       "4 -0.007361   0.000054  1358.16 -0.011956  ...   6.4     0.0  9275.276   \n",
       "\n",
       "   R_USGDP_t  VIX_t  Unnamed: 41  Unnamed: 42  Unnamed: 43  Unnamed: 44  \\\n",
       "0        0.0  26.38          NaN          NaN          NaN          NaN   \n",
       "1        0.0  26.62          NaN          NaN          NaN          NaN   \n",
       "2        0.0  27.93          NaN          NaN          NaN          NaN   \n",
       "3        0.0  27.19          NaN          NaN          NaN          NaN   \n",
       "4        0.0  28.95          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 45  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df = pd.read_excel(r'\\path\\to\\DSML_Predictors_final.xlsx', sheet_name = 'Predictors (täglich)',header = 4, usecols=\"A:AT\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc0717bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Find rows with missing value and delete them\"\"\"\n",
    "delete_rows = []\n",
    "for i in range(0, len(df)):\n",
    "    for j in range(1,1):\n",
    "        if(np.isnan(df.iloc[i][j]) or df.iloc[i][j] == 0):\n",
    "            delete_rows.append(i)\n",
    "            break  \n",
    "#print(\"Zeilen mit folgenden Indices werden gelöscht:\", delete_rows)\n",
    "df.drop(labels=delete_rows, axis=0, inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cb8ac11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Class_p   RABS_WIL_t      R_WIL_t     SQR_WIL_t      R_DAX_t  \\\n",
      "count  8044.000000  8044.000000  8044.000000  8.044000e+03  8044.000000   \n",
      "mean      1.439334     0.000403     0.000148  2.385927e-05     0.000131   \n",
      "std       0.570678     0.011220     0.004883  8.997339e-05     0.005999   \n",
      "min       1.000000    -0.122876    -0.056939  0.000000e+00    -0.056697   \n",
      "25%       1.000000    -0.003936    -0.001713  6.461532e-07    -0.002499   \n",
      "50%       1.000000     0.000400     0.000174  4.157272e-06     0.000214   \n",
      "75%       2.000000     0.005287     0.002290  1.802552e-05     0.003091   \n",
      "max       3.000000     0.113758     0.046791  3.242064e-03     0.046893   \n",
      "\n",
      "          R_FTSE_t    R_USDUK_t      R_COP_t      R_USD_t      R_S&P_t  ...  \\\n",
      "count  8044.000000  8044.000000  8044.000000  8044.000000  8044.000000  ...   \n",
      "mean      0.000066    -0.000018     0.000072     0.000007     0.000142  ...   \n",
      "std       0.004760     0.002543     0.006757     0.002153     0.004891  ...   \n",
      "min      -0.049998    -0.036099    -0.045494    -0.013309    -0.055439  ...   \n",
      "25%      -0.002172    -0.001364    -0.003277    -0.001171    -0.001749  ...   \n",
      "50%       0.000028     0.000000     0.000000     0.000000     0.000125  ...   \n",
      "75%       0.002460     0.001364     0.003496     0.001192     0.002355  ...   \n",
      "max       0.040756     0.019432     0.050925     0.012392     0.047586  ...   \n",
      "\n",
      "       R_USPYC30Y_t    R_LIB3M_t   R_USTB3M_t     R_INFL_t     R_USDJ_t  \\\n",
      "count   8044.000000  8044.000000  8044.000000  8044.000000  8044.000000   \n",
      "mean      -0.000779    -0.000926    -0.000787     0.000071     0.000139   \n",
      "std        0.049819     0.029542     0.044141     0.085927     0.003924   \n",
      "min       -0.305100    -0.420000    -1.370000    -2.590000    -0.071256   \n",
      "25%       -0.029700    -0.001250     0.000000     0.000000     0.000000   \n",
      "50%        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%        0.027225     0.002500     0.000000     0.000000     0.000000   \n",
      "max        0.300700     0.570000     0.580000     2.020000     0.048586   \n",
      "\n",
      "        R_USFFTR_t       R_UR_t    R_USGDP_t        VIX_t   Class  \n",
      "count  8044.000000  8044.000000  8044.000000  8044.000000  8044.0  \n",
      "mean     -0.000178    -0.000018     0.000040    19.342113     2.0  \n",
      "std       0.013772     0.006688     0.000734     8.063761     0.0  \n",
      "min      -0.845098    -0.084321    -0.040659     9.140000     2.0  \n",
      "25%       0.000000     0.000000     0.000000    13.620000     2.0  \n",
      "50%       0.000000     0.000000     0.000000    17.330000     2.0  \n",
      "75%       0.000000     0.000000     0.000000    22.640000     2.0  \n",
      "max       0.301030     0.526809     0.031600    82.690000     2.0  \n",
      "\n",
      "[8 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Dataframe mit relevanten Spalten erstellen\"\"\"\n",
    "df_final = df[['DATE', 'Class_p', 'RABS_WIL_t','R_WIL_t','SQR_WIL_t','R_DAX_t','R_FTSE_t','R_USDUK_t','R_COP_t', 'R_USD_t', 'R_S&P_t', 'R_USPYC1Y_t', 'R_USPYC10Y_t', 'R_USPYC30Y_t', 'R_LIB3M_t', 'R_USTB3M_t', 'R_INFL_t', 'R_USDJ_t', 'R_USFFTR_t', 'R_UR_t', 'R_USGDP_t', 'VIX_t']]\n",
    "df_final['R_WIL_t'] = df_final['R_WIL_t'].astype(float)\n",
    "df_final.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_final.dropna(inplace=True)\n",
    "df_final.loc[:,'Class'] = 2.0\n",
    "print(df_final.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "381142a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>Class_p</th>\n",
       "      <th>RABS_WIL_t</th>\n",
       "      <th>R_WIL_t</th>\n",
       "      <th>SQR_WIL_t</th>\n",
       "      <th>R_DAX_t</th>\n",
       "      <th>R_FTSE_t</th>\n",
       "      <th>R_USDUK_t</th>\n",
       "      <th>R_COP_t</th>\n",
       "      <th>R_USD_t</th>\n",
       "      <th>...</th>\n",
       "      <th>R_USPYC30Y_t</th>\n",
       "      <th>R_LIB3M_t</th>\n",
       "      <th>R_USTB3M_t</th>\n",
       "      <th>R_INFL_t</th>\n",
       "      <th>R_USDJ_t</th>\n",
       "      <th>R_USFFTR_t</th>\n",
       "      <th>R_UR_t</th>\n",
       "      <th>R_USGDP_t</th>\n",
       "      <th>VIX_t</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991-01-01 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.38</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991-01-02 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.010247</td>\n",
       "      <td>-0.004473</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.010096</td>\n",
       "      <td>-0.003091</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.009793</td>\n",
       "      <td>-0.001466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1200</td>\n",
       "      <td>-0.01563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.62</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1991-01-03 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.012005</td>\n",
       "      <td>-0.005245</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>-0.002148</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>-0.002137</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0302</td>\n",
       "      <td>-0.18750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.93</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1991-01-04 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.002608</td>\n",
       "      <td>-0.001134</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>-0.002910</td>\n",
       "      <td>-0.009739</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1084</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991-01-07 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.016808</td>\n",
       "      <td>-0.007361</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>-0.011956</td>\n",
       "      <td>-0.002623</td>\n",
       "      <td>-0.006221</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0966</td>\n",
       "      <td>0.20313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.95</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DATE  Class_p  RABS_WIL_t   R_WIL_t  SQR_WIL_t   R_DAX_t  \\\n",
       "0  1991-01-01 00:00:00      1.0    0.000000  0.000000   0.000000  0.000000   \n",
       "1  1991-01-02 00:00:00      1.0   -0.010247 -0.004473   0.000020 -0.010096   \n",
       "2  1991-01-03 00:00:00      1.0   -0.012005 -0.005245   0.000028  0.000175   \n",
       "3  1991-01-04 00:00:00      1.0   -0.002608 -0.001134   0.000001  0.009250   \n",
       "4  1991-01-07 00:00:00      1.0   -0.016808 -0.007361   0.000054 -0.011956   \n",
       "\n",
       "   R_FTSE_t  R_USDUK_t   R_COP_t   R_USD_t  ...  R_USPYC30Y_t  R_LIB3M_t  \\\n",
       "0  0.000000   0.000000  0.000000  0.000000  ...        0.0000    0.00000   \n",
       "1 -0.003091   0.002804  0.009793 -0.001466  ...       -0.1200   -0.01563   \n",
       "2 -0.002148   0.000893 -0.002137  0.000105  ...       -0.0302   -0.18750   \n",
       "3  0.001699  -0.002910 -0.009739  0.002875  ...        0.1084    0.00000   \n",
       "4 -0.002623  -0.006221  0.003804  0.006772  ...        0.0966    0.20313   \n",
       "\n",
       "   R_USTB3M_t  R_INFL_t  R_USDJ_t  R_USFFTR_t  R_UR_t  R_USGDP_t  VIX_t  Class  \n",
       "0         0.0       0.0       0.0         0.0     0.0        0.0  26.38    2.0  \n",
       "1         0.0       0.0       0.0         0.0     0.0        0.0  26.62    2.0  \n",
       "2         0.0       0.0       0.0         0.0     0.0        0.0  27.93    2.0  \n",
       "3         0.0       0.0       0.0         0.0     0.0        0.0  27.19    2.0  \n",
       "4         0.0       0.0       0.0         0.0     0.0        0.0  28.95    2.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7f7edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lags für Return WIL\n",
    "for i in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    df_final['R_WIL_t-'+str(i)] = df['R_WIL_t'].shift(i)\n",
    "    df_final['SQR_WIL_t-'+str(i)] = df['SQR_WIL_t'].shift(i)\n",
    "    df_final['R_DAX_t-'+str(i)] = df['R_DAX_t'].shift(i)\n",
    "    df_final['R_FTSE_t-'+str(i)] = df['R_FTSE_t'].shift(i)\n",
    "    df_final['R_USDUK_t-'+str(i)] = df['R_USDUK_t'].shift(i)\n",
    "    df_final['R_COP_t-'+str(i)] = df['R_COP_t'].shift(i)\n",
    "    df_final['R_USD_t-'+str(i)] = df['R_USD_t'].shift(i)\n",
    "    df_final['R_S&P_t-'+str(i)] = df['R_S&P_t'].shift(i)\n",
    "    df_final['R_USPYC1Y_t-'+str(i)] = df['R_USPYC1Y_t'].shift(i)\n",
    "    df_final['R_USPYC10Y_t-'+str(i)] = df['R_USPYC10Y_t'].shift(i)\n",
    "    df_final['R_USPYC30Y_t-'+str(i)] = df['R_USPYC30Y_t'].shift(i)\n",
    "    df_final['R_LIB3M_t-'+str(i)] = df['R_LIB3M_t'].shift(i)\n",
    "    df_final['R_USTB3M_t-'+str(i)] = df['R_USTB3M_t'].shift(i)\n",
    "    df_final['R_INFL_t-'+str(i)] = df['R_INFL_t'].shift(i)\n",
    "    df_final['R_USDJ_t-'+str(i)] = df['R_USDJ_t'].shift(i)\n",
    "    df_final['R_USFFTR_t-'+str(i)] = df['R_USFFTR_t'].shift(i)\n",
    "    df_final['R_UR_t-'+str(i)] = df['R_UR_t'].shift(i)\n",
    "    df_final['R_USGDP_t-'+str(i)] = df['R_USGDP_t'].shift(i)\n",
    "    df_final['VIX_t-'+str(i)] = df['VIX_t'].shift(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0571bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dropna(inplace=True)\n",
    "df_final.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78a3729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubDataframe(dataframe, end_year):\n",
    "    \n",
    "    df_sub = pd.DataFrame(columns=list(dataframe))\n",
    "    df_sub_train = pd.DataFrame(columns=list(dataframe))\n",
    "    df_sub_test = pd.DataFrame(columns=list(dataframe))\n",
    "    for i in range(0, len(dataframe)):\n",
    "        \n",
    "        year = dataframe.loc[i,'DATE'].year\n",
    "        \n",
    "        if year <= end_year:\n",
    "            \n",
    "            df_sub = df_sub.append(dataframe.loc[i,:])\n",
    "            \n",
    "            if year <= (end_year - 2):\n",
    "                \n",
    "                df_sub_train = df_sub_train.append(dataframe.loc[i,:])\n",
    "                \n",
    "            if (year > (end_year -2)) and (year <= end_year):\n",
    "                \n",
    "                df_sub_test = df_sub_test.append(dataframe.loc[i,:])\n",
    "        \n",
    "    return df_sub, df_sub_train, df_sub_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80b164c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2009, 2011, 2013, 2015, 2017, 2019, 2021]\n"
     ]
    }
   ],
   "source": [
    "Perioden = [2009,2011,2013,2015,2017,2019,2021]\n",
    "print(Perioden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ad2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erstellen der Dataframes für jede Periode\n",
    "for i in Perioden:\n",
    "    globals()[f\"df_periode_{i}\"],globals()[f\"df_train_periode_{i}\"],globals()[f\"df_test_periode_{i}\"] = getSubDataframe(df_final, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c61e9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Perioden:\n",
    "    globals()[f\"df_test_periode_{i}\"].reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca4cb00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>Class_p</th>\n",
       "      <th>RABS_WIL_t</th>\n",
       "      <th>R_WIL_t</th>\n",
       "      <th>SQR_WIL_t</th>\n",
       "      <th>R_DAX_t</th>\n",
       "      <th>R_FTSE_t</th>\n",
       "      <th>R_USDUK_t</th>\n",
       "      <th>R_COP_t</th>\n",
       "      <th>R_USD_t</th>\n",
       "      <th>...</th>\n",
       "      <th>R_USPYC10Y_t-10</th>\n",
       "      <th>R_USPYC30Y_t-10</th>\n",
       "      <th>R_LIB3M_t-10</th>\n",
       "      <th>R_USTB3M_t-10</th>\n",
       "      <th>R_INFL_t-10</th>\n",
       "      <th>R_USDJ_t-10</th>\n",
       "      <th>R_USFFTR_t-10</th>\n",
       "      <th>R_UR_t-10</th>\n",
       "      <th>R_USGDP_t-10</th>\n",
       "      <th>VIX_t-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0398</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.00550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>0.003353</td>\n",
       "      <td>1.124130e-05</td>\n",
       "      <td>0.004465</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>-0.001971</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.01975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.006221</td>\n",
       "      <td>-0.002710</td>\n",
       "      <td>7.344346e-06</td>\n",
       "      <td>-0.005445</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>-0.003854</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>-0.0113</td>\n",
       "      <td>0.00700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>2.118374e-06</td>\n",
       "      <td>-0.003038</td>\n",
       "      <td>-0.002690</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>-0.000763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>0.01188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.002484</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>1.166958e-06</td>\n",
       "      <td>0.003291</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>-0.001306</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0183</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>0.01387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2021-10-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>5.225709e-06</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2021-10-26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>1.072123e-07</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>-0.004999</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0296</td>\n",
       "      <td>-0.0562</td>\n",
       "      <td>0.00500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2021-10-27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.007019</td>\n",
       "      <td>-0.003059</td>\n",
       "      <td>9.357940e-06</td>\n",
       "      <td>-0.001415</td>\n",
       "      <td>-0.001456</td>\n",
       "      <td>-0.002463</td>\n",
       "      <td>-0.012771</td>\n",
       "      <td>-0.000694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0295</td>\n",
       "      <td>-0.0601</td>\n",
       "      <td>-0.00300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.011038</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>2.272747e-05</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.008387</td>\n",
       "      <td>-0.002089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0347</td>\n",
       "      <td>-0.0231</td>\n",
       "      <td>-0.00150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2021-10-29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>3.866138e-07</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>-0.002889</td>\n",
       "      <td>-0.002076</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.00138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          DATE  Class_p  RABS_WIL_t   R_WIL_t     SQR_WIL_t   R_DAX_t  \\\n",
       "0   2020-01-01      1.0    0.000000  0.000000  0.000000e+00  0.000000   \n",
       "1   2020-01-02      2.0    0.007750  0.003353  1.124130e-05  0.004465   \n",
       "2   2020-01-03      1.0   -0.006221 -0.002710  7.344346e-06 -0.005445   \n",
       "3   2020-01-06      2.0    0.003357  0.001455  2.118374e-06 -0.003038   \n",
       "4   2020-01-07      1.0   -0.002484 -0.001080  1.166958e-06  0.003291   \n",
       "..         ...      ...         ...       ...           ...       ...   \n",
       "473 2021-10-25      2.0    0.005278  0.002286  5.225709e-06  0.001569   \n",
       "474 2021-10-26      2.0    0.000754  0.000327  1.072123e-07  0.004372   \n",
       "475 2021-10-27      2.0   -0.007019 -0.003059  9.357940e-06 -0.001415   \n",
       "476 2021-10-28      2.0    0.011038  0.004767  2.272747e-05 -0.000262   \n",
       "477 2021-10-29      2.0    0.001433  0.000622  3.866138e-07 -0.000209   \n",
       "\n",
       "     R_FTSE_t  R_USDUK_t   R_COP_t   R_USD_t  ...  R_USPYC10Y_t-10  \\\n",
       "0    0.000000   0.000000  0.000000  0.000000  ...           0.0398   \n",
       "1    0.003547  -0.001971  0.001023  0.002068  ...          -0.0031   \n",
       "2    0.001032  -0.003854 -0.004213 -0.000045  ...           0.0040   \n",
       "3   -0.002690   0.002947  0.000746 -0.000763  ...           0.0033   \n",
       "4   -0.000085  -0.001306  0.000922  0.001480  ...          -0.0183   \n",
       "..        ...        ...       ...       ...  ...              ...   \n",
       "473  0.001100  -0.000095  0.009317  0.000788  ...           0.0000   \n",
       "474  0.003283   0.000678 -0.004999  0.000648  ...          -0.0296   \n",
       "475 -0.001456  -0.002463 -0.012771 -0.000694  ...          -0.0295   \n",
       "476 -0.000228   0.002715  0.008387 -0.002089  ...          -0.0347   \n",
       "477 -0.000713  -0.002889 -0.002076  0.003568  ...           0.0598   \n",
       "\n",
       "     R_USPYC30Y_t-10  R_LIB3M_t-10  R_USTB3M_t-10  R_INFL_t-10  R_USDJ_t-10  \\\n",
       "0             0.0369       0.00550            0.0          0.0          0.0   \n",
       "1             0.0144       0.01975            0.0          0.0          0.0   \n",
       "2            -0.0113       0.00700            0.0          0.0          0.0   \n",
       "3            -0.0045       0.01188            0.0          0.0          0.0   \n",
       "4            -0.0022       0.01387            0.0          0.0          0.0   \n",
       "..               ...           ...            ...          ...          ...   \n",
       "473           0.0000       0.00062            0.0          0.0          0.0   \n",
       "474          -0.0562       0.00500            0.0          0.0          0.0   \n",
       "475          -0.0601      -0.00300            0.0          0.0          0.0   \n",
       "476          -0.0231      -0.00150            0.0          0.0          0.0   \n",
       "477           0.0289       0.00138            0.0          0.0          0.0   \n",
       "\n",
       "     R_USFFTR_t-10  R_UR_t-10  R_USGDP_t-10  VIX_t-10  \n",
       "0              0.0        0.0           0.0     12.58  \n",
       "1              0.0        0.0           0.0     12.50  \n",
       "2              0.0        0.0           0.0     12.51  \n",
       "3              0.0        0.0           0.0     12.61  \n",
       "4              0.0        0.0           0.0     12.67  \n",
       "..             ...        ...           ...       ...  \n",
       "473            0.0        0.0           0.0     20.00  \n",
       "474            0.0        0.0           0.0     19.85  \n",
       "475            0.0        0.0           0.0     18.64  \n",
       "476            0.0        0.0           0.0     16.86  \n",
       "477            0.0        0.0           0.0     16.30  \n",
       "\n",
       "[478 rows x 213 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_periode_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5be1b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Perioden:\n",
    "    globals()[f\"quantile_25_periode_{i}\"] = globals()[f\"df_train_periode_{i}\"]['RABS_WIL_t'].quantile(.25)\n",
    "    globals()[f\"quantile_75_periode_{i}\"] = globals()[f\"df_train_periode_{i}\"]['RABS_WIL_t'].quantile(.75)\n",
    "    globals()[f\"quantile_33_periode_{i}\"] = globals()[f\"df_train_periode_{i}\"]['RABS_WIL_t'].quantile(.33)\n",
    "    globals()[f\"quantile_67_periode_{i}\"] = globals()[f\"df_train_periode_{i}\"]['RABS_WIL_t'].quantile(.67)\n",
    "    globals()[f\"quantile_20_periode_{i}\"] = globals()[f\"df_train_periode_{i}\"]['RABS_WIL_t'].quantile(.20)\n",
    "    globals()[f\"quantile_80_periode_{i}\"] = globals()[f\"df_train_periode_{i}\"]['RABS_WIL_t'].quantile(.80)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c23690af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Perioden:\n",
    "    for j in range(0, len(globals()[f\"df_periode_{i}\"])):\n",
    "        if globals()[f\"df_periode_{i}\"].loc[j,'RABS_WIL_t'] <= globals()[f\"quantile_25_periode_{i}\"]:\n",
    "            globals()[f\"df_periode_{i}\"].loc[j, 'Class'] = 1.0\n",
    "        elif globals()[f\"df_periode_{i}\"].loc[j,'RABS_WIL_t'] > globals()[f\"quantile_75_periode_{i}\"]:\n",
    "            globals()[f\"df_periode_{i}\"].loc[j,'Class'] = 3.0\n",
    "        else:\n",
    "            globals()[f\"df_periode_{i}\"].loc[j,'Class'] = 2.0\n",
    "            \n",
    "for i in Perioden:\n",
    "    for j in range(0, len(globals()[f\"df_train_periode_{i}\"])):\n",
    "        if globals()[f\"df_train_periode_{i}\"].loc[j,'RABS_WIL_t'] <= globals()[f\"quantile_25_periode_{i}\"]:\n",
    "            globals()[f\"df_train_periode_{i}\"].loc[j, 'Class'] = 1.0\n",
    "        elif globals()[f\"df_train_periode_{i}\"].loc[j,'RABS_WIL_t'] > globals()[f\"quantile_75_periode_{i}\"]:\n",
    "            globals()[f\"df_train_periode_{i}\"].loc[j,'Class'] = 3.0\n",
    "        else:\n",
    "            globals()[f\"df_train_periode_{i}\"].loc[j,'Class'] = 2.0\n",
    "\n",
    "for i in Perioden:\n",
    "    for j in range(0, len(globals()[f\"df_test_periode_{i}\"])):\n",
    "        if (globals()[f\"df_test_periode_{i}\"].loc[j,'RABS_WIL_t'] <= globals()[f\"quantile_25_periode_{i}\"]):\n",
    "            globals()[f\"df_test_periode_{i}\"].loc[j, 'Class'] = 1.0\n",
    "        elif globals()[f\"df_test_periode_{i}\"].loc[j,'RABS_WIL_t'] > globals()[f\"quantile_75_periode_{i}\"]:\n",
    "            globals()[f\"df_test_periode_{i}\"].loc[j,'Class'] = 3.0\n",
    "        else:\n",
    "            globals()[f\"df_test_periode_{i}\"].loc[j,'Class'] = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e06d2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "\n",
    "for i in Perioden:\n",
    "    # Train and test data\n",
    "    globals()[f\"x_periode_{i}\"] = globals()[f\"df_periode_{i}\"].drop(['DATE', 'Class', 'Class_p', 'RABS_WIL_t','R_WIL_t','SQR_WIL_t','R_DAX_t','R_FTSE_t','R_USDUK_t','R_COP_t', 'R_USD_t', 'R_S&P_t', 'R_USPYC1Y_t', 'R_USPYC10Y_t', 'R_USPYC30Y_t', 'R_LIB3M_t', 'R_USTB3M_t', 'R_INFL_t', 'R_USDJ_t', 'R_USFFTR_t', 'R_UR_t', 'R_USGDP_t', 'VIX_t'], axis=1)\n",
    "    globals()[f\"y_periode_{i}\"] = globals()[f\"df_periode_{i}\"]['Class']\n",
    "    globals()[f\"x_train_periode_{i}\"] = globals()[f\"df_train_periode_{i}\"].drop(['DATE', 'Class', 'Class_p', 'RABS_WIL_t','R_WIL_t','SQR_WIL_t','R_DAX_t','R_FTSE_t','R_USDUK_t','R_COP_t', 'R_USD_t', 'R_S&P_t', 'R_USPYC1Y_t', 'R_USPYC10Y_t', 'R_USPYC30Y_t', 'R_LIB3M_t', 'R_USTB3M_t', 'R_INFL_t', 'R_USDJ_t', 'R_USFFTR_t', 'R_UR_t', 'R_USGDP_t', 'VIX_t'], axis=1)\n",
    "    globals()[f\"x_test_periode_{i}\"] = globals()[f\"df_test_periode_{i}\"].drop(['DATE', 'Class', 'Class_p', 'RABS_WIL_t','R_WIL_t','SQR_WIL_t','R_DAX_t','R_FTSE_t','R_USDUK_t','R_COP_t', 'R_USD_t', 'R_S&P_t', 'R_USPYC1Y_t', 'R_USPYC10Y_t', 'R_USPYC30Y_t', 'R_LIB3M_t', 'R_USTB3M_t', 'R_INFL_t', 'R_USDJ_t', 'R_USFFTR_t', 'R_UR_t', 'R_USGDP_t', 'VIX_t'], axis=1)\n",
    "    globals()[f\"y_train_periode_{i}\"] = globals()[f\"df_train_periode_{i}\"]['Class']\n",
    "    globals()[f\"y_test_periode_{i}\"] = globals()[f\"df_test_periode_{i}\"]['Class']\n",
    "\n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(globals()[f\"y_periode_{i}\"])\n",
    "    globals()[f\"encoded_y_periode_{i}\"] = encoder.transform(globals()[f\"y_periode_{i}\"])\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    globals()[f\"dummy_y_periode_{i}\"] = np_utils.to_categorical(globals()[f\"encoded_y_periode_{i}\"])\n",
    "    \n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(globals()[f\"y_train_periode_{i}\"])\n",
    "    globals()[f\"encoded_y_train_periode_{i}\"] = encoder.transform(globals()[f\"y_train_periode_{i}\"])\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    globals()[f\"dummy_y_train_periode_{i}\"] = np_utils.to_categorical(globals()[f\"encoded_y_train_periode_{i}\"])\n",
    "\n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(globals()[f\"y_test_periode_{i}\"])\n",
    "    globals()[f\"encoded_y_test_periode_{i}\"] = encoder.transform(globals()[f\"y_test_periode_{i}\"])\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    globals()[f\"dummy_y_test_periode_{i}\"] = np_utils.to_categorical(globals()[f\"encoded_y_test_periode_{i}\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e29549b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### scale the variables: (noch machen)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SC= StandardScaler().fit(globals()[f\"x_train_periode_{i}\"][list(globals()[f\"x_train_periode_{i}\"])])\n",
    "globals()[f\"x_train_periode_{i}\"][list(globals()[f\"x_train_periode_{i}\"])]=SC.transform(globals()[f\"x_train_periode_{i}\"][list(globals()[f\"x_train_periode_{i}\"])])\n",
    "globals()[f\"x_test_periode_{i}\"][list(globals()[f\"x_train_periode_{i}\"])]=SC.transform(globals()[f\"x_test_periode_{i}\"][list(globals()[f\"x_train_periode_{i}\"])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e98a9f",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73fa9d",
   "metadata": {},
   "source": [
    "In order to get the optimal hyperparameters for each period for each model, we use a Grid Search Method. Therefore, we need to define the hyperparameters on a particualr range. Due to a high time required for computing, we used the default values from the Paper Nevasalmi(2020). Especially for GBM we needed to use a littel amount of possible hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5958c12",
   "metadata": {},
   "source": [
    "Definition of the five model estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "797ba92a",
   "metadata": {},
   "outputs": [],
   "source": [
    " estimator_KNN = KNeighborsClassifier()\n",
    " estimator_GBM = GradientBoostingClassifier()      \n",
    " estimator_SVM = svm.SVC()\n",
    " estimator_NN = Sequential()\n",
    " estimator_RF = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1ecb7",
   "metadata": {},
   "source": [
    "Definition of the parameters for K-Nearest Neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d4c7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    " grid_params_KNN = {\n",
    "     'n_neighbors': [250,331,400],\n",
    "     'leaf_size': [20, 30, 40],\n",
    "     'weights': ['uniform', 'distance'],\n",
    "     'metric': ['euclidean', 'manhattan','minkowski']\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd7230",
   "metadata": {},
   "source": [
    "Definition of the parameters for Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc07df94",
   "metadata": {},
   "outputs": [],
   "source": [
    " grid_params_GBM = {\n",
    "         'n_estimators': [50,100],\n",
    "         'max_depth':[2,3],\n",
    "         'min_samples_split':[2,4],\n",
    "         'learning_rate':[0.005,0.001],\n",
    "         'n_iter_no_change' : [800,931]\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f90967",
   "metadata": {},
   "source": [
    "Definition of the parameters for Support Vector Machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5456283",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params_SVM = {\n",
    "     \n",
    "     'kernel' : ['poly'], # default kernel='rbf'                                                \n",
    "     'degree' : [3] , #default=3\n",
    "     'gamma'  : ['scale'], #default='scale'   \n",
    "     'C' : [0.1, 0.3, 0.5, 1.0] #default=1.0  \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd065b07",
   "metadata": {},
   "source": [
    "Definition of the parameters for Neural Networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59f78476",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params_NN = {\n",
    "    'neuron_hidden' : [5,10,20],\n",
    "    'activation_hidden' : ['sigmoid', 'tanh'],\n",
    "    'optimizer' : ['adam', 'sgd'],\n",
    "    'dropout' : [0.1,0.3],\n",
    "    'weight_decay' : [0.1,0.3,0.6]\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61f803",
   "metadata": {},
   "source": [
    "Definition of the parameters for Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adeca3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    " grid_params_RF= {\n",
    "     'n_estimators': [100,300,500],\n",
    "     'max_features': ['auto', 'log2'],\n",
    "     'min_samples_leaf': [1, 2, 3]\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab544e2",
   "metadata": {},
   "source": [
    "Visualization of the results of the Grid-Search-Method for K-Nearest Neighbor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a91a0a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Periode 2009\n",
      "Best: 0.479774 using {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.479774 (0.049828) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.477966 (0.055138) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.475932 (0.049302) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.473898 (0.054292) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.478644 (0.045150) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.475932 (0.047755) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.475254 (0.051163) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.475480 (0.053418) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.475706 (0.053066) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.473672 (0.054338) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.479548 (0.045623) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.472090 (0.054535) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.479774 (0.049828) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.477966 (0.055138) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.475932 (0.049302) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.473898 (0.054292) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.478644 (0.045150) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.475932 (0.047755) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.479774 (0.049828) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.477966 (0.055138) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.475932 (0.049302) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.473898 (0.054292) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.478644 (0.045150) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.475932 (0.047755) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.475254 (0.051163) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.475480 (0.053418) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.475706 (0.053066) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.473672 (0.054338) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.479548 (0.045623) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.472090 (0.054535) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.479774 (0.049828) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.477966 (0.055138) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.475932 (0.049302) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.473898 (0.054292) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.478644 (0.045150) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.475932 (0.047755) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.479774 (0.049828) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.477966 (0.055138) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.475932 (0.049302) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.473898 (0.054292) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.478644 (0.045150) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.475932 (0.047755) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.475254 (0.051163) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.475480 (0.053418) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.475706 (0.053066) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.473672 (0.054338) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.479548 (0.045623) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.472090 (0.054535) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.479774 (0.049828) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.477966 (0.055138) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.475932 (0.049302) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.473898 (0.054292) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.478644 (0.045150) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.475932 (0.047755) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Periode 2011\n",
      "Best: 0.485454 using {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.476358 (0.056023) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.473530 (0.059657) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.483836 (0.051223) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.479593 (0.054834) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.485454 (0.046396) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.483230 (0.048643) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.475955 (0.055662) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.474339 (0.059022) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.481006 (0.049838) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.478784 (0.054498) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.481007 (0.048690) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.483433 (0.050147) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.476358 (0.056023) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.473530 (0.059657) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.483836 (0.051223) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.479593 (0.054834) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.485454 (0.046396) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.483230 (0.048643) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.476358 (0.056023) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.473530 (0.059657) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.483836 (0.051223) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.479593 (0.054834) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.485454 (0.046396) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.483230 (0.048643) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.475955 (0.055662) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.474339 (0.059022) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.481006 (0.049838) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.478784 (0.054498) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.481007 (0.048690) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.483433 (0.050147) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.476358 (0.056023) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.473530 (0.059657) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.483836 (0.051223) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.479593 (0.054834) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.485454 (0.046396) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.483230 (0.048643) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.476358 (0.056023) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.473530 (0.059657) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.483836 (0.051223) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.479593 (0.054834) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.485454 (0.046396) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.483230 (0.048643) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.475955 (0.055662) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.474339 (0.059022) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.481006 (0.049838) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.478784 (0.054498) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.481007 (0.048690) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.483433 (0.050147) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.476358 (0.056023) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.473530 (0.059657) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.483836 (0.051223) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.479593 (0.054834) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.485454 (0.046396) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.483230 (0.048643) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periode 2013\n",
      "Best: 0.473768 using {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.469013 (0.071738) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.465903 (0.072798) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.469743 (0.067016) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.468646 (0.069959) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.470476 (0.068139) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.470109 (0.067502) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.469014 (0.074512) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.468647 (0.075325) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.469197 (0.072384) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.470842 (0.070509) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.473768 (0.064931) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.471024 (0.066215) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.469013 (0.071738) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.465903 (0.072798) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.469743 (0.067016) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.468646 (0.069959) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.470476 (0.068139) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.470109 (0.067502) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.469013 (0.071738) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.465903 (0.072798) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.469743 (0.067016) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.468646 (0.069959) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.470476 (0.068139) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.470109 (0.067502) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.469014 (0.074512) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.468647 (0.075325) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.469197 (0.072384) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.470842 (0.070509) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.473768 (0.064931) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.471024 (0.066215) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.469013 (0.071738) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.465903 (0.072798) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.469743 (0.067016) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.468646 (0.069959) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.470476 (0.068139) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.470109 (0.067502) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.469013 (0.071738) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.465903 (0.072798) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.469743 (0.067016) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.468646 (0.069959) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.470476 (0.068139) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.470109 (0.067502) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.469014 (0.074512) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.468647 (0.075325) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.469197 (0.072384) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.470842 (0.070509) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.473768 (0.064931) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.471024 (0.066215) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.469013 (0.071738) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.465903 (0.072798) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.469743 (0.067016) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.468646 (0.069959) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.470476 (0.068139) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.470109 (0.067502) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Periode 2015\n",
      "Best: 0.481719 using {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.481219 (0.062393) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.479048 (0.061806) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.479549 (0.063179) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.478714 (0.065653) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.479215 (0.060751) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.478381 (0.063180) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.479549 (0.064326) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.480050 (0.064665) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.480718 (0.060905) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.480050 (0.062114) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.481719 (0.058604) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.480718 (0.061997) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.481219 (0.062393) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.479048 (0.061806) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.479549 (0.063179) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.478714 (0.065653) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.479215 (0.060751) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.478381 (0.063180) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.481219 (0.062393) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.479048 (0.061806) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.479549 (0.063179) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.478714 (0.065653) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.479215 (0.060751) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.478381 (0.063180) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.479549 (0.064326) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.480050 (0.064665) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.480718 (0.060905) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.480050 (0.062114) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.481719 (0.058604) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.480718 (0.061997) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.481219 (0.062393) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.479048 (0.061806) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.479549 (0.063179) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.478714 (0.065653) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.479215 (0.060751) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.478381 (0.063180) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.481219 (0.062393) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.479048 (0.061806) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.479549 (0.063179) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.478714 (0.065653) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.479215 (0.060751) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.478381 (0.063180) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.479549 (0.064326) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.480050 (0.064665) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.480718 (0.060905) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.480050 (0.062114) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.481719 (0.058604) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.480718 (0.061997) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.481219 (0.062393) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.479048 (0.061806) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.479549 (0.063179) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.478714 (0.065653) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.479215 (0.060751) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.478381 (0.063180) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periode 2017\n",
      "Best: 0.493943 using {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.491947 (0.040359) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.492869 (0.041370) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.493943 (0.039783) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.491181 (0.043598) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.491641 (0.041494) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.492563 (0.044319) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.490565 (0.039654) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.488569 (0.042028) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.493943 (0.037928) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.490720 (0.042213) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.491794 (0.040034) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.491180 (0.042621) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.491947 (0.040359) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.492869 (0.041370) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.493943 (0.039783) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.491181 (0.043598) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.491641 (0.041494) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.492563 (0.044319) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.491947 (0.040359) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.492869 (0.041370) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.493943 (0.039783) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.491181 (0.043598) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.491641 (0.041494) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.492563 (0.044319) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.490565 (0.039654) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.488569 (0.042028) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.493943 (0.037928) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.490720 (0.042213) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.491794 (0.040034) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.491180 (0.042621) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.491947 (0.040359) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.492869 (0.041370) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.493943 (0.039783) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.491181 (0.043598) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.491641 (0.041494) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.492563 (0.044319) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.491947 (0.040359) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.492869 (0.041370) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.493943 (0.039783) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.491181 (0.043598) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.491641 (0.041494) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.492563 (0.044319) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.490565 (0.039654) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.488569 (0.042028) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.493943 (0.037928) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.490720 (0.042213) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.491794 (0.040034) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.491180 (0.042621) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.491947 (0.040359) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.492869 (0.041370) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.493943 (0.039783) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.491181 (0.043598) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.491641 (0.041494) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.492563 (0.044319) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Periode 2019\n",
      "Best: 0.509953 using {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.504693 (0.013458) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.505404 (0.012432) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.508816 (0.014046) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.505830 (0.012059) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.509953 (0.014389) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.507678 (0.011873) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.505404 (0.010890) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.505404 (0.010348) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.509526 (0.012455) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.506825 (0.010288) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.508389 (0.014346) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.506399 (0.012827) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.504693 (0.013458) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.505404 (0.012432) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.508816 (0.014046) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.505830 (0.012059) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.509953 (0.014389) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.507678 (0.011873) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.504693 (0.013458) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.505404 (0.012432) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.508816 (0.014046) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.505830 (0.012059) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.509953 (0.014389) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.507678 (0.011873) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.505404 (0.010890) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.505404 (0.010348) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.509526 (0.012455) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.506825 (0.010288) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.508389 (0.014346) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.506399 (0.012827) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.504693 (0.013458) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.505404 (0.012432) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.508816 (0.014046) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.505830 (0.012059) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.509953 (0.014389) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.507678 (0.011873) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.504693 (0.013458) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.505404 (0.012432) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.508816 (0.014046) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.505830 (0.012059) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.509953 (0.014389) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.507678 (0.011873) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.505404 (0.010890) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.505404 (0.010348) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.509526 (0.012455) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.506825 (0.010288) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.508389 (0.014346) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.506399 (0.012827) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.504693 (0.013458) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.505404 (0.012432) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.508816 (0.014046) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.505830 (0.012059) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.509953 (0.014389) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.507678 (0.011873) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periode 2021\n",
      "Best: 0.503177 using {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.502912 (0.004432) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.503044 (0.005304) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.501324 (0.002195) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.501853 (0.003240) with: {'leaf_size': 20, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.502912 (0.004432) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.503177 (0.004951) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.500794 (0.001176) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.500927 (0.001426) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.500529 (0.000713) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.500529 (0.000713) with: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.502912 (0.004432) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.503044 (0.005304) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.501324 (0.002195) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.501853 (0.003240) with: {'leaf_size': 20, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.502912 (0.004432) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.503044 (0.005304) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.501324 (0.002195) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.501853 (0.003240) with: {'leaf_size': 30, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.502912 (0.004432) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.503177 (0.004951) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.500794 (0.001176) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.500927 (0.001426) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.500529 (0.000713) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.500529 (0.000713) with: {'leaf_size': 30, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.502912 (0.004432) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.503044 (0.005304) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.501324 (0.002195) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.501853 (0.003240) with: {'leaf_size': 30, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.502912 (0.004432) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.503044 (0.005304) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.501324 (0.002195) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.501853 (0.003240) with: {'leaf_size': 40, 'metric': 'euclidean', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.502912 (0.004432) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.503177 (0.004951) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.500794 (0.001176) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.500927 (0.001426) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.500529 (0.000713) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.500529 (0.000713) with: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 400, 'weights': 'distance'}\n",
      "0.502912 (0.004432) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'uniform'}\n",
      "0.503044 (0.005304) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 250, 'weights': 'distance'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'uniform'}\n",
      "0.502250 (0.003734) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 331, 'weights': 'distance'}\n",
      "0.501324 (0.002195) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'uniform'}\n",
      "0.501853 (0.003240) with: {'leaf_size': 40, 'metric': 'minkowski', 'n_neighbors': 400, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "for i in Perioden:\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "         estimator = estimator_KNN,\n",
    "         param_grid = grid_params_KNN,\n",
    "         verbose = 1,\n",
    "         cv = 5,\n",
    "         n_jobs = -1,\n",
    "         scoring = 'accuracy'\n",
    "         )\n",
    "    #print(\"Periode \" + str(i))\n",
    "    #globals()[f\"gs_results_Periode_{i}\"] = gs.fit(globals()[f\"x_train_periode_{i}\"], globals()[f\"y_train_periode_{i}\"])\n",
    "    #print(globals()[f\"gs_results_Periode_{i}\"].best_estimator_)\n",
    "    \n",
    "    globals()[f\"grid_result_KNN_periode_{i}\"] = gs.fit(globals()[f\"x_train_periode_{i}\"], globals()[f\"y_train_periode_{i}\"])\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Periode \" + str(i))\n",
    "    print(\"Best: %f using %s\" % (globals()[f\"grid_result_KNN_periode_{i}\"].best_score_, \n",
    "                                 globals()[f\"grid_result_KNN_periode_{i}\"].best_params_))\n",
    "    globals()[f\"means_periode_{i}\"] = globals()[f\"grid_result_KNN_periode_{i}\"].cv_results_['mean_test_score']\n",
    "    globals()[f\"stds_periode_{i}\"] = globals()[f\"grid_result_KNN_periode_{i}\"].cv_results_['std_test_score']\n",
    "    globals()[f\"params_periode_{i}\"] = globals()[f\"grid_result_KNN_periode_{i}\"].cv_results_['params']\n",
    "    for mean, stdev, param in zip(globals()[f\"means_periode_{i}\"], globals()[f\"stds_periode_{i}\"], globals()[f\"params_periode_{i}\"]):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72de0a",
   "metadata": {},
   "source": [
    "Visualization of the results of the Grid-Search-Method for Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824cfe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in Perioden:     \n",
    "    gs = GridSearchCV(\n",
    "         estimator = estimator_GBM,\n",
    "         param_grid = grid_params_GBM,\n",
    "         verbose = 1,\n",
    "         cv = 5,\n",
    "         n_jobs = -1,\n",
    "         scoring = 'accuracy'\n",
    "         )\n",
    "\n",
    "    #print(\"Periode \" + str(i))\n",
    "    #globals()[f\"gs_results_Periode_{i}\"] = gs.fit(globals()[f\"x_train_periode_{i}\"], globals()[f\"y_train_periode_{i}\"])\n",
    "    #print(globals()[f\"gs_results_Periode_{i}\"].best_estimator_)\n",
    "    \n",
    "    globals()[f\"grid_result_GBM_periode_{i}\"] = gs.fit(globals()[f\"x_train_periode_{i}\"], globals()[f\"y_train_periode_{i}\"])\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Periode \" + str(i))\n",
    "    print(\"Best: %f using %s\" % (globals()[f\"grid_result_GBM_periode_{i}\"].best_score_, \n",
    "                                 globals()[f\"grid_result_GBM_periode_{i}\"].best_params_))\n",
    "    globals()[f\"means_periode_{i}\"] = globals()[f\"grid_result_GBM_periode_{i}\"].cv_results_['mean_test_score']\n",
    "    globals()[f\"stds_periode_{i}\"] = globals()[f\"grid_result_GBM_periode_{i}\"].cv_results_['std_test_score']\n",
    "    globals()[f\"params_periode_{i}\"] = globals()[f\"grid_result_GBM_periode_{i}\"].cv_results_['params']\n",
    "    for mean, stdev, param in zip(globals()[f\"means_periode_{i}\"], globals()[f\"stds_periode_{i}\"], globals()[f\"params_periode_{i}\"]):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fe82b",
   "metadata": {},
   "source": [
    "Visualization of the results of the Grid-Search-Method for the Support Vector Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cca4ff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Periode 2009\n",
      "Best: 0.501243 using {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.501243 (0.011131) with: {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.499661 (0.011149) with: {'C': 0.3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.498983 (0.010172) with: {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.499887 (0.009389) with: {'C': 1.0, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Periode 2011\n",
      "Best: 0.511319 using {'C': 0.3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.510309 (0.016815) with: {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.511319 (0.019087) with: {'C': 0.3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.510308 (0.018465) with: {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.509903 (0.018476) with: {'C': 1.0, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Periode 2013\n",
      "Best: 0.513263 using {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.513263 (0.019378) with: {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.511800 (0.018608) with: {'C': 0.3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.511251 (0.017436) with: {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.511068 (0.017078) with: {'C': 1.0, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Periode 2015\n",
      "Best: 0.510768 using {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.510768 (0.010177) with: {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.507596 (0.008700) with: {'C': 0.3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.507262 (0.009769) with: {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.506595 (0.009541) with: {'C': 1.0, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Periode 2017\n",
      "Best: 0.511902 using {'C': 0.3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.510366 (0.011747) with: {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.511902 (0.011244) with: {'C': 0.3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.511595 (0.011422) with: {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.510981 (0.010821) with: {'C': 1.0, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Periode 2019\n",
      "Best: 0.510377 using {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.510377 (0.009262) with: {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.509241 (0.007241) with: {'C': 0.3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.509951 (0.008984) with: {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.510094 (0.008299) with: {'C': 1.0, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Periode 2021\n",
      "Best: 0.513633 using {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.506221 (0.011130) with: {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.512442 (0.015370) with: {'C': 0.3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.513633 (0.014737) with: {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.513103 (0.011938) with: {'C': 1.0, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "for i in Perioden:\n",
    "    gs = GridSearchCV(\n",
    "         estimator = estimator_SVM,\n",
    "         param_grid = grid_params_SVM,\n",
    "         verbose = 1,\n",
    "         cv = 5,\n",
    "         n_jobs = -1,\n",
    "         scoring = 'accuracy'\n",
    "         )\n",
    "\n",
    "    globals()[f\"grid_result_SVM_periode_{i}\"] = gs.fit(globals()[f\"x_train_periode_{i}\"], globals()[f\"y_train_periode_{i}\"])\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Periode \" + str(i))\n",
    "    print(\"Best: %f using %s\" % (globals()[f\"grid_result_SVM_periode_{i}\"].best_score_, \n",
    "                                 globals()[f\"grid_result_SVM_periode_{i}\"].best_params_))\n",
    "    globals()[f\"means_periode_{i}\"] = globals()[f\"grid_result_SVM_periode_{i}\"].cv_results_['mean_test_score']\n",
    "    globals()[f\"stds_periode_{i}\"] = globals()[f\"grid_result_SVM_periode_{i}\"].cv_results_['std_test_score']\n",
    "    globals()[f\"params_periode_{i}\"] = globals()[f\"grid_result_SVM_periode_{i}\"].cv_results_['params']\n",
    "    for mean, stdev, param in zip(globals()[f\"means_periode_{i}\"], globals()[f\"stds_periode_{i}\"], globals()[f\"params_periode_{i}\"]):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3d609",
   "metadata": {},
   "source": [
    "Visualization of the results of the Grid-Search-Method for Neural Networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8974f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                1910      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,943\n",
      "Trainable params: 1,943\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n",
      "Periode 2009\n",
      "Best: 0.512994 using {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.497401 (0.114831) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.502373 (0.112493) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.507345 (0.108519) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.501469 (0.114811) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.505085 (0.109964) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.505537 (0.108922) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.512994 (0.104623) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.500565 (0.117560) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.343051 (0.113692) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.508927 (0.109201) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.506893 (0.107266) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.504181 (0.112395) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.498531 (0.115423) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.501243 (0.115240) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.507571 (0.108659) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.509379 (0.108145) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499435 (0.116449) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.512090 (0.104064) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.502599 (0.114638) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.509605 (0.100860) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.512542 (0.103071) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499435 (0.116449) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499887 (0.116969) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "<keras.wrappers.scikit_learn.KerasClassifier object at 0x000001F24C037CD0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 5)                 955       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 973\n",
      "Trainable params: 973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n",
      "Periode 2011\n",
      "Best: 0.517975 using {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.501193 (0.110511) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.505031 (0.103549) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.506654 (0.106507) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.507867 (0.106261) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.513324 (0.101148) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.506856 (0.108604) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.503216 (0.108187) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.506249 (0.107950) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.507864 (0.104385) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.504024 (0.106859) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.488670 (0.101638) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.503418 (0.108077) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.510293 (0.106196) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.507058 (0.106337) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.496142 (0.104856) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.517567 (0.095988) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.515751 (0.099476) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.512921 (0.103344) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.517773 (0.098809) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.516762 (0.102417) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.514136 (0.103999) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.503418 (0.108077) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.502407 (0.108642) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.515346 (0.101714) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.504228 (0.104009) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.517167 (0.101411) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.505238 (0.107148) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.517975 (0.100654) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.509482 (0.101987) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.504227 (0.108812) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.514741 (0.101284) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.515953 (0.102513) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.513529 (0.103715) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.516557 (0.096363) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.516561 (0.102519) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.503015 (0.106697) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.501395 (0.108238) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499374 (0.109642) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499980 (0.110137) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "<keras.wrappers.scikit_learn.KerasClassifier object at 0x000001F24C0D8850>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 10)                1910      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,943\n",
      "Trainable params: 1,943\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n",
      "Periode 2013\n",
      "Best: 0.521284 using {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.500068 (0.122828) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.503362 (0.119340) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.498057 (0.120272) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.501348 (0.123225) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.502447 (0.120215) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.500800 (0.123049) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.508852 (0.114566) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.502264 (0.120105) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.505009 (0.117821) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.502081 (0.120571) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.501349 (0.122168) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.516348 (0.106655) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.512324 (0.114036) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.511043 (0.111504) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.487077 (0.137744) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.500434 (0.123539) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499702 (0.122965) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.513787 (0.109736) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.510862 (0.109118) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.504824 (0.120464) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499337 (0.122445) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.520188 (0.104031) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.516531 (0.105248) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.505557 (0.118620) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499154 (0.121767) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.517263 (0.105412) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.516897 (0.106495) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.505738 (0.117850) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.521284 (0.102655) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.511230 (0.113328) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.504277 (0.118487) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.500983 (0.121659) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.516712 (0.105007) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.511229 (0.113589) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499885 (0.122776) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "<keras.wrappers.scikit_learn.KerasClassifier object at 0x000001F254D6CBB0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 20)                3820      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 63        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,883\n",
      "Trainable params: 3,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n",
      "Periode 2015\n",
      "Best: 0.519087 using {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.500055 (0.093779) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.508736 (0.092100) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.500055 (0.093779) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.502058 (0.092740) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.511908 (0.089959) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.505063 (0.092054) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.500222 (0.094085) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.500890 (0.093357) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.515080 (0.084377) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.516081 (0.083722) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.512242 (0.086495) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.500556 (0.093226) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.504395 (0.089523) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.516582 (0.086523) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.508903 (0.089671) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.459320 (0.129095) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.519087 (0.082752) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.514913 (0.087362) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.512910 (0.086626) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.509070 (0.086526) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.515581 (0.085862) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.507567 (0.091544) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.513577 (0.086721) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.513410 (0.088343) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.506232 (0.091892) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.517751 (0.083535) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.514746 (0.090681) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.508903 (0.092177) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499888 (0.093473) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "<keras.wrappers.scikit_learn.KerasClassifier object at 0x000001F256DF3B80>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 20)                3820      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 63        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,883\n",
      "Trainable params: 3,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n",
      "Periode 2017\n",
      "Best: 0.516810 using {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.501914 (0.089613) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.505752 (0.089249) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.509589 (0.086458) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.506522 (0.087394) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.500070 (0.090738) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.508978 (0.085788) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.500378 (0.089080) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.515887 (0.083414) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.513123 (0.083495) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.514965 (0.083395) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.514352 (0.084484) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.513431 (0.084137) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.515733 (0.082036) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.495005 (0.095194) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.516810 (0.082210) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.511127 (0.085336) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.508979 (0.086800) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.487946 (0.104031) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.513738 (0.083915) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.511741 (0.083936) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.512049 (0.086129) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.501607 (0.089793) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.513738 (0.084637) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.514966 (0.082798) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.507443 (0.088452) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.503447 (0.088608) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.514506 (0.084974) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.509438 (0.084728) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.511435 (0.085799) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.505443 (0.087569) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499917 (0.090849) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "<keras.wrappers.scikit_learn.KerasClassifier object at 0x000001F256DF33A0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 5)                 955       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 973\n",
      "Trainable params: 973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n",
      "Periode 2019\n",
      "Best: 0.520057 using {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.504705 (0.102061) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.498877 (0.104272) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.505985 (0.101659) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.501720 (0.103438) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.501578 (0.105235) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499588 (0.105363) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.500014 (0.105490) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.515935 (0.091550) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.520057 (0.088449) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.509396 (0.096212) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.513234 (0.096685) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.514940 (0.094208) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.510817 (0.096082) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499303 (0.104251) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.502289 (0.103156) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.517072 (0.092265) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.517072 (0.092264) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.509822 (0.097991) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.498308 (0.107320) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.514655 (0.091330) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.514228 (0.092310) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.515650 (0.093169) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.517498 (0.092187) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.513234 (0.094034) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.502146 (0.103092) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.514512 (0.092576) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.513802 (0.094135) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.510960 (0.095026) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499872 (0.105632) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "<keras.wrappers.scikit_learn.KerasClassifier object at 0x000001F2586F5CA0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 10)                1910      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,943\n",
      "Trainable params: 1,943\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n",
      "Periode 2021\n",
      "Best: 0.514938 using {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'sigmoid', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.513879 (0.088305) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.508452 (0.092293) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499849 (0.097787) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.514938 (0.088379) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.500378 (0.097698) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.507129 (0.093485) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.501305 (0.097003) with: {'activation_hidden': 'tanh', 'dropout': 0.1, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.509114 (0.090258) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.501702 (0.096796) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.500113 (0.097886) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.500246 (0.097791) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 5, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.509776 (0.092870) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.500246 (0.097791) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 10, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "0.502496 (0.096173) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.1}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.3}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'adam', 'weight_decay': 0.6}\n",
      "0.499981 (0.097980) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.1}\n",
      "0.500113 (0.097886) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.3}\n",
      "0.499981 (0.097915) with: {'activation_hidden': 'tanh', 'dropout': 0.3, 'neuron_hidden': 20, 'optimizer': 'sgd', 'weight_decay': 0.6}\n",
      "<keras.wrappers.scikit_learn.KerasClassifier object at 0x000001F260FEECD0>\n"
     ]
    }
   ],
   "source": [
    "for i in Perioden:\n",
    "\n",
    "    # define baseline model\n",
    "    def neural_model(neuron_hidden=10, activation_hidden='tanh', optimizer='adam', learning_rate=0.001, dropout=0.3, weight_decay=0.2):\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(neuron_hidden, input_shape=(190,), activation=activation_hidden, use_bias=True, kernel_regularizer=regularizers.l1(weight_decay)))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(3, activation='softmax', use_bias=True))\n",
    "        model.summary()\n",
    "        #optimizer1 = optimizer\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model \n",
    "\n",
    "\n",
    "\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    estimator = KerasClassifier(build_fn=neural_model, \n",
    "                                epochs=100, \n",
    "                                batch_size=300, \n",
    "                                verbose=3)\n",
    "\n",
    "\n",
    "\n",
    "    grid = GridSearchCV(estimator=estimator, \n",
    "                        scoring= 'accuracy',\n",
    "                        param_grid=grid_params_NN,\n",
    "                        n_jobs=-1,\n",
    "                        cv=5)\n",
    "\n",
    "    \n",
    "    globals()[f\"grid_result_NN_periode_{i}\"] = grid.fit(globals()[f\"x_train_periode_{i}\"], globals()[f\"y_train_periode_{i}\"])\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Periode \" + str(i))\n",
    "    print(\"Best: %f using %s\" % (globals()[f\"grid_result_NN_periode_{i}\"].best_score_, \n",
    "                                 globals()[f\"grid_result_NN_periode_{i}\"].best_params_))\n",
    "    globals()[f\"means_periode_{i}\"] = globals()[f\"grid_result_NN_periode_{i}\"].cv_results_['mean_test_score']\n",
    "    globals()[f\"stds_periode_{i}\"] = globals()[f\"grid_result_NN_periode_{i}\"].cv_results_['std_test_score']\n",
    "    globals()[f\"params_periode_{i}\"] = globals()[f\"grid_result_NN_periode_{i}\"].cv_results_['params']\n",
    "    for mean, stdev, param in zip(globals()[f\"means_periode_{i}\"], globals()[f\"stds_periode_{i}\"], globals()[f\"params_periode_{i}\"]):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0017a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5186440677966101"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result_periode_2009.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f40560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_hidden': 'tanh',\n",
       " 'dropout': 0.3,\n",
       " 'neuron_hidden': 5,\n",
       " 'optimizer': 'adam',\n",
       " 'weight_decay': 0.1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result_periode_2009.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd477cc9",
   "metadata": {},
   "source": [
    "Visualization of the results of the Grid-Search-Method for Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5726aed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Periode 2009\n",
      "Best: 0.471412 using {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.465989 (0.053696) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.468475 (0.053285) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.461017 (0.066814) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.465763 (0.062739) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.464181 (0.062175) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.465085 (0.058812) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.468249 (0.059930) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.470960 (0.054717) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.466441 (0.058121) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "0.465311 (0.056603) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.467797 (0.056239) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.470960 (0.050706) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.470056 (0.051255) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.468249 (0.057659) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.471412 (0.052454) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.468927 (0.056961) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.469605 (0.052328) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.468249 (0.053910) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Periode 2011\n",
      "Best: 0.472319 using {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.464031 (0.070245) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.468479 (0.073785) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.463024 (0.079918) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.466661 (0.075927) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.468279 (0.075142) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.465448 (0.073800) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.464034 (0.079732) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.464841 (0.074443) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.465651 (0.077477) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "0.467263 (0.061671) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.471913 (0.069288) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.472318 (0.068521) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.463223 (0.075170) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.472319 (0.071705) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.470299 (0.069777) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.470298 (0.066543) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.471510 (0.068174) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.470501 (0.072067) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Periode 2013\n",
      "Best: 0.466999 using {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.457309 (0.082208) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.458405 (0.083442) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.464258 (0.075927) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.464255 (0.074122) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.459870 (0.080427) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.460234 (0.080970) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.464804 (0.073691) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.464624 (0.080348) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.464807 (0.078993) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "0.466999 (0.075439) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.462244 (0.080183) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.461514 (0.078186) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.465172 (0.078508) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.461881 (0.081453) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.458405 (0.083356) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.465722 (0.084333) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.464989 (0.078543) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.462611 (0.076612) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Periode 2015\n",
      "Best: 0.483222 using {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.473873 (0.074199) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.477713 (0.069264) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.478380 (0.072502) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.476210 (0.068315) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.481719 (0.067114) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.477045 (0.066169) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.476545 (0.075218) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.475376 (0.071474) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.476878 (0.071176) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "0.471535 (0.068860) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.477712 (0.067949) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.479716 (0.066245) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.483222 (0.068532) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.475375 (0.070968) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.476044 (0.070146) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.479048 (0.072975) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.478047 (0.072813) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.477880 (0.067199) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Periode 2017\n",
      "Best: 0.498858 using {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "0.496552 (0.038833) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.497631 (0.051966) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.497475 (0.044384) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.495326 (0.046890) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.498399 (0.049677) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.492255 (0.044667) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.497936 (0.046638) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.497782 (0.042052) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.498858 (0.041741) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "0.493635 (0.048657) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.497013 (0.040074) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.496707 (0.043458) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.494404 (0.046050) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.491332 (0.035606) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.493943 (0.042746) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.491793 (0.041757) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.498857 (0.044174) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.496554 (0.043880) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periode 2019\n",
      "Best: 0.510094 using {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.499718 (0.013186) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.506257 (0.015135) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.507962 (0.018753) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.502135 (0.011508) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.510094 (0.019443) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.507109 (0.013082) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.505262 (0.009503) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.503982 (0.014702) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.507679 (0.014312) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "0.507251 (0.011640) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.505261 (0.012317) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.509668 (0.019761) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.503272 (0.017683) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.506968 (0.015498) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.507536 (0.014592) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.499860 (0.013928) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.506825 (0.015808) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.508247 (0.011912) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Periode 2021\n",
      "Best: 0.514426 using {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.499470 (0.017117) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.514294 (0.017638) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.512308 (0.013375) with: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.505690 (0.018037) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.512309 (0.013264) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.508867 (0.016276) with: {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.508867 (0.015911) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.510059 (0.018383) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.511382 (0.015566) with: {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "0.503838 (0.019721) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.507412 (0.017102) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.510059 (0.017017) with: {'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.502646 (0.014188) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.513499 (0.015420) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.514426 (0.020493) with: {'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.508867 (0.011558) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.511912 (0.014345) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.510853 (0.015735) with: {'max_features': 'log2', 'min_samples_leaf': 3, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "for i in Perioden:\n",
    "    gs = GridSearchCV(\n",
    "         estimator = estimator_RF,\n",
    "         param_grid = grid_params_RF,\n",
    "         verbose = 1,\n",
    "         cv = 5,\n",
    "         n_jobs = -1,\n",
    "         scoring = 'accuracy'\n",
    "         )\n",
    "\n",
    "    #print(\"Periode \" + str(i))\n",
    "    #globals()[f\"gs_results_Periode_{i}\"] = gs.fit(globals()[f\"x_train_periode_{i}\"], globals()[f\"y_train_periode_{i}\"])\n",
    "    #print(globals()[f\"gs_results_Periode_{i}\"].best_estimator_)\n",
    "    \n",
    "    globals()[f\"grid_result_RF_periode_{i}\"] = gs.fit(globals()[f\"x_train_periode_{i}\"], globals()[f\"y_train_periode_{i}\"])\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Periode \" + str(i))\n",
    "    print(\"Best: %f using %s\" % (globals()[f\"grid_result_RF_periode_{i}\"].best_score_, \n",
    "                                 globals()[f\"grid_result_RF_periode_{i}\"].best_params_))\n",
    "    globals()[f\"means_periode_{i}\"] = globals()[f\"grid_result_RF_periode_{i}\"].cv_results_['mean_test_score']\n",
    "    globals()[f\"stds_periode_{i}\"] = globals()[f\"grid_result_RF_periode_{i}\"].cv_results_['std_test_score']\n",
    "    globals()[f\"params_periode_{i}\"] = globals()[f\"grid_result_RF_periode_{i}\"].cv_results_['params']\n",
    "    for mean, stdev, param in zip(globals()[f\"means_periode_{i}\"], globals()[f\"stds_periode_{i}\"], globals()[f\"params_periode_{i}\"]):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b16642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
